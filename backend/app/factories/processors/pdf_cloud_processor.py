"""
PDF Processor - Gemini integration with PRODUCTION HARDENING.

SECURITY: PII protection, Secret sanitization, DoS mitigation
PERFORMANCE: Async processing, Retry logic, Content size limits
OBSERVABILITY: Structured logging for monitoring
"""

from __future__ import annotations

import asyncio
import hashlib
import io
import logging
import os
import shutil
import time
from pathlib import Path
from typing import Final, Optional

from google import genai
from google.genai import types
from pydantic import BaseModel, Field, ValidationError

from app.core.database import SessionLocal
from app.core.exceptions import (ConfigurationError, ExternalDependencyError,
                                 TechnicalError)
from app.core.settings import get_settings
from app.factories.processors.base import FileProcessor, ProcessedDocument
from app.services.settings_service import SettingsService

logger = logging.getLogger(__name__)


# ========== CUSTOM EXCEPTIONS (P2) ==========
class GeminiProcessorError(TechnicalError):
    """Base exception for Gemini processing errors."""

    pass


# ========== TYPED METADATA (P2) ==========
class PdfMetadata(BaseModel):
    """Strongly typed metadata for PDF files."""

    file_type: str = "pdf"
    file_name_hash: str  # SHA256 hash (P0: No PII)
    # page_count: int = Field(ge=0, le=10000) # Gemini might not return page count easily, optional
    source_tool: str = "Gemini"
    truncated: bool = False
    content_size_bytes: int = Field(ge=0)


# ========== PROCESSOR CLASS ==========
class PdfCloudProcessor(FileProcessor):
    """
    PDF processor using Google Gemini API with PRODUCTION HARDENING.

    CRITICAL FIXES:
    - P0: PII Protection (hashed filenames)
    - P0: Content Size Limits (prevent OOM)
    - P0: Secret Sanitization (no API key leaks)
    - P1: Non-blocking IO
    - P1: Retry Logic / Robust Error Handling
    """

    # Configuration (P2: Inject from settings)
    _settings = get_settings()
    MAX_FILE_SIZE_MB: Final[int] = 100
    MAX_CONTENT_SIZE_MB: Final[int] = 50  # P0: Prevent memory DoS
    PARSE_TIMEOUT_SECONDS: Final[float] = 300.0

    # Gemini Safety Configuration
    POLLING_INTERVAL_SECONDS = 2
    MAX_POLLING_DURATION_SECONDS = 300

    # Prompt for text extraction
    EXTRACTION_PROMPT = """
    Extract all text content from this PDF document, including all tables and handwritten text. 
    Return ONLY the raw text content in Markdown format. 
    Do not add any conversational preamble or conclusion.
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize PDF processor.

        Args:
            api_key: Gemini API key (optional, reads from settings if None)
        """
        super().__init__(max_file_size_bytes=self.MAX_FILE_SIZE_MB * 1024 * 1024)

        # P0: Never log API key directly
        self._api_key = api_key or get_settings().GEMINI_API_KEY

        if self._api_key:
            self.client = genai.Client(api_key=self._api_key)
            logger.info("pdf_processor_initialized", extra={"provider": "gemini", "has_api_key": True})
        else:
            self.client = None
            logger.warning("pdf_processor_initialized_without_api_key", extra={"provider": "gemini"})

    async def process(self, file_path: str | Path) -> list[ProcessedDocument]:
        """
        Process PDF with full production hardening using Gemini.

        Returns:
            List with single ProcessedDocument
        """
        file_hash: Optional[str] = None

        try:
            # P0: Early API key validation
            if not self._api_key or not self.client:
                raise ConfigurationError("GEMINI_API_KEY is missing. Cannot process PDF.")

            # Validate file (async, inherited)
            validated_path = await self._validate_file_path(file_path)

            # P0: Hash filename for secure logging
            file_hash = self._hash_filename(validated_path.name)
            file_extension = validated_path.suffix.lower()

            # CACHE: Check if we already extracted this PDF (based on mtime + size)
            import json
            import os

            cache_dir = Path(".cache/pdf_extraction")
            cache_dir.mkdir(parents=True, exist_ok=True)
            cache_file = cache_dir / f"{file_hash}.json"

            file_stat = validated_path.stat()
            file_signature = f"{file_stat.st_mtime}_{file_stat.st_size}"

            if cache_file.exists():
                try:
                    cached = json.loads(cache_file.read_text(encoding="utf-8"))
                    if cached.get("signature") == file_signature:
                        logger.info(f"pdf_cache_hit | Hash: {file_hash}")
                        # Reconstruct ProcessedDocument from cache
                        results = []
                        for page in cached.get("pages", []):
                            meta = page.get("metadata", {})
                            meta["file_path"] = str(validated_path)
                            results.append(ProcessedDocument(content=page["content"], metadata=meta, success=True))
                        return results
                except Exception as e:
                    logger.warning(f"pdf_cache_read_fail | {e}")

            logger.info(
                "pdf_processing_started",
                extra={
                    "file_hash": file_hash,
                    "extension": file_extension,
                    "provider": "gemini",
                    "max_timeout": self.PARSE_TIMEOUT_SECONDS,
                },
            )

            # Process with Gemini (with timeout)
            # Returns list of dicts: [{"page_number": 1, "content": "..."}]
            pages_data = await asyncio.wait_for(
                self._process_with_gemini(str(validated_path), file_hash), timeout=self.PARSE_TIMEOUT_SECONDS
            )

            # P0: Enforce strict type check
            if not isinstance(pages_data, list):
                logger.warning("gemini_invalid_format", extra={"file_hash": file_hash, "type": str(type(pages_data))})
                return self._create_error_document("Gemini returned invalid format")

            results = []
            max_bytes = self.MAX_CONTENT_SIZE_MB * 1024 * 1024

            for page in pages_data:
                content = page.get("content", "")
                page_num = page.get("page_number", 0)

                # Truncation check per page
                content_bytes = len(content.encode("utf-8"))
                truncated = False

                if content_bytes > max_bytes:
                    encoded = content.encode("utf-8")[:max_bytes]
                    content = encoded.decode("utf-8", errors="ignore")
                    truncated = True

                # Metadata per page
                pdf_metadata = PdfMetadata(
                    file_name_hash=file_hash,
                    source_tool="Gemini",
                    truncated=truncated,
                    content_size_bytes=len(content.encode("utf-8")),
                )

                # P2: Flatten metadata for frontend usage
                meta_dict = pdf_metadata.model_dump()
                meta_dict["page_number"] = page_num
                # FIX: Ensure file_path is preserved for file opening
                meta_dict["file_path"] = str(validated_path)

                results.append(ProcessedDocument(content=content, metadata=meta_dict, success=True))

            if not results:
                logger.warning("gemini_no_pages_returned", extra={"file_hash": file_hash})
                return self._create_error_document("No content extracted")

            logger.info("pdf_processing_completed", extra={"file_hash": file_hash, "pages_extracted": len(results)})

            # CACHE: Save extraction results
            try:
                cache_data = {
                    "signature": file_signature,
                    "pages": [
                        {"content": r.content, "metadata": {k: v for k, v in r.metadata.items() if k != "file_path"}}
                        for r in results
                    ],
                }
                cache_file.write_text(json.dumps(cache_data, ensure_ascii=False), encoding="utf-8")
                logger.debug(f"pdf_cache_saved | Hash: {file_hash}")
            except Exception as e:
                logger.warning(f"pdf_cache_write_fail | {e}")

            return results

        except asyncio.TimeoutError:
            logger.error(
                "pdf_processing_timeout", extra={"file_hash": file_hash, "timeout_seconds": self.PARSE_TIMEOUT_SECONDS}
            )
            return self._create_error_document("PDF parsing timed out")

        except ConfigurationError as e:
            logger.error("pdf_config_error", extra={"file_hash": file_hash})
            return self._create_error_document(str(e))

        except FileNotFoundError:
            logger.error("pdf_file_not_found", extra={"file_hash": file_hash})
            return self._create_error_document("File not found")

        except GeminiProcessorError as e:
            logger.error("gemini_error", extra={"file_hash": file_hash, "error_type": type(e).__name__}, exc_info=True)
            return self._create_error_document(f"Gemini error: {str(e)}")

        except Exception as e:
            logger.error("pdf_unexpected_error", extra={"file_hash": file_hash}, exc_info=True)
            return self._create_error_document(f"Unexpected error: {str(e)}")

    async def _process_with_gemini(self, file_path: str, file_hash: str) -> str:
        """
        Uploads and processes the PDF file using Gemini.
        Uses in-memory BytesIO with safe name to avoid UnicodeEncodeError and Temp Files.
        """
        gemini_file = None

        try:
            # 1. Read File into Memory (Non-blocking)
            # LIMITATION: Loads 100MB max into RAM. Acceptable given User constraints against temp files.
            file_size = await asyncio.to_thread(os.path.getsize, file_path)
            logger.debug(f"reading_file_to_memory | Hash: {file_hash} | Size: {file_size}")

            def read_file_to_buffer():
                with open(file_path, "rb") as f:
                    return io.BytesIO(f.read())

            file_buffer = await asyncio.to_thread(read_file_to_buffer)

            # P0: Fix UnicodeEncodeError by giving buffer a safe ASCII name
            safe_name = f"doc_{file_hash}.pdf"
            file_buffer.name = safe_name  # Hack: SDK/httpx uses .name if present

            # 2. Upload (Non-blocking)
            # SDK should accept file-like object (BytesIO)
            def upload_safe():
                # We must rewind buffer if needed, but it's new
                return self.client.files.upload(
                    file=file_buffer, config={"display_name": safe_name, "mime_type": "application/pdf"}
                )

            gemini_file = await asyncio.to_thread(upload_safe)

            # 3. Wait for Processing
            start_time = time.time()
            while gemini_file.state.name == "PROCESSING":
                if time.time() - start_time > self.MAX_POLLING_DURATION_SECONDS:
                    raise GeminiProcessorError("Gemini file processing timed out")

                await asyncio.sleep(self.POLLING_INTERVAL_SECONDS)
                gemini_file = await asyncio.to_thread(self.client.files.get, name=gemini_file.name)

            if gemini_file.state.name == "FAILED":
                raise GeminiProcessorError(f"Gemini failed to process file: {gemini_file.error.message}")

            # 4. Generate Content (Extract Text)
            # Fetch model from settings dynamically
            async with SessionLocal() as db:
                from app.core.settings import settings

                settings_service = SettingsService(db)
                model_name = await settings_service.get_value("gemini_chat_model", settings.GEMINI_CHAT_MODEL)

            # JSON Schema for strict page separation
            prompt = """
            Extract all text content from this PDF document, page by page.
            Return a JSON List of objects, where each object represents a page.
            Format: [{"page_number": 1, "content": "Full text of page 1..."}, ...]
            Include all tables and handwritten text in the 'content' field as Markdown.
            """

            # TRACING: Wrap Gemini call for Phoenix visibility
            from opentelemetry import trace

            tracer = trace.get_tracer(__name__)

            with tracer.start_as_current_span(
                "gemini_pdf_extraction",
                attributes={
                    "llm.vendor": "Google",
                    "llm.request.model": model_name,
                    "llm.request.max_tokens": 8192,
                    "llm.request.temperature": 0.0,
                    "llm.input_messages": str([{"role": "user", "content": prompt[:200] + "..."}]),  # Convert to string
                    "file.hash": file_hash,
                },
            ) as span:
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model=model_name,
                    contents=[prompt, gemini_file],
                    config=types.GenerateContentConfig(
                        temperature=0.0, max_output_tokens=8192, top_p=0.95, response_mime_type="application/json"
                    ),
                )

                # Add response metrics to span
                if response.text:
                    span.set_attribute("llm.response.length", len(response.text))
                    # Estimate token count (rough: 1 token â‰ˆ 4 chars)
                    estimated_tokens = len(response.text) // 4
                    span.set_attribute("llm.usage.total_tokens", estimated_tokens)
                    span.set_attribute("llm.usage.completion_tokens", estimated_tokens)

            import json
            import re

            # DEBUG: Print raw response to help debug parsing errors
            logger.debug(f"DEBUG GEMINI RESPONSE (Hash: {file_hash}): {response.text}")

            text_to_parse = response.text.strip()

            # 1. Attempt basic clean of Markdown fences
            if "```json" in text_to_parse:
                text_to_parse = text_to_parse.split("```json")[1].split("```")[0].strip()
            elif "```" in text_to_parse:
                text_to_parse = text_to_parse.split("```")[1].split("```")[0].strip()

            try:
                # Strategy A: Direct Load
                pages_data = json.loads(text_to_parse)
            except json.JSONDecodeError:
                # Strategy B: JSON Stream / Concatenated Objects (NDJSON-like)
                # If Gemini returned { ... } { ... } instead of [ ... ]
                pages_data = []
                decoder = json.JSONDecoder()
                pos = 0
                while pos < len(text_to_parse):
                    # Skip whitespace
                    while pos < len(text_to_parse) and text_to_parse[pos].isspace():
                        pos += 1
                    if pos >= len(text_to_parse):
                        break

                    try:
                        obj, idx = decoder.raw_decode(text_to_parse[pos:])
                        pages_data.append(obj)
                        pos += idx
                    except json.JSONDecodeError:
                        # If we fail in the middle, we might be hitting garbage or truncation
                        # Try to skip to next '{' or '['
                        next_brace = text_to_parse.find("{", pos + 1)
                        if next_brace != -1:
                            pos = next_brace
                        else:
                            break  # Give up if no more objects

            # Post-processing checks
            if isinstance(pages_data, list):
                # Filter out non-dicts (sanity check)
                return [p for p in pages_data if isinstance(p, dict)]
            elif isinstance(pages_data, dict):
                return [pages_data]

            logger.warning("gemini_json_parse_empty", extra={"file_hash": file_hash})
            return []

        except Exception as e:
            if isinstance(e, GeminiProcessorError):
                raise e
            raise GeminiProcessorError(f"Gemini operation failed: {str(e)}") from e

        finally:
            # 5. Cleanup (Remote file only)
            if gemini_file:
                # Fire and forget deletion
                asyncio.create_task(self._safe_delete(gemini_file.name))

    async def _safe_delete(self, file_name: str):
        """Non-blocking background deletion."""
        if not self.client:
            return
        try:
            await asyncio.to_thread(self.client.files.delete, name=file_name)
        except Exception as e:
            logger.warning(f"cleanup_fail | Gemini file: {file_name} | Error: {e}")

    @staticmethod
    def _hash_filename(filename: str) -> str:
        """Hash filename for secure logging (P0: PII Protection)."""
        return hashlib.sha256(filename.encode()).hexdigest()[:16]

    @staticmethod
    def _create_error_document(error_msg: str) -> list[ProcessedDocument]:
        """Helper to create standardized error response."""
        return [
            ProcessedDocument(
                content="",
                metadata={"file_type": "pdf", "source_tool": "Gemini"},
                success=False,
                error_message=error_msg,
            )
        ]

    def get_supported_extensions(self) -> list[str]:
        """Supported extensions."""
        return ["pdf"]
